<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Akasaki Research - 工具箱的魔法部日志</title><meta name="author" content="Miya Akasaki"><meta name="copyright" content="Miya Akasaki"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="工具箱的魔法部日志">
<meta property="og:type" content="website">
<meta property="og:title" content="Akasaki Research">
<meta property="og:url" content="https://research.akasaki.space/index.html">
<meta property="og:site_name" content="Akasaki Research">
<meta property="og:description" content="工具箱的魔法部日志">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://research.akasaki.space/img/visualdust.png">
<meta property="article:author" content="Miya Akasaki">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://research.akasaki.space/img/visualdust.png"><link rel="shortcut icon" href="/img/logo.svg"><link rel="canonical" href="https://research.akasaki.space/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Miya Akasaki","link":"链接: ","source":"来源: Akasaki Research","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Akasaki Research',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2021-07-20 22:54:31'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/visualdust.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/background.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Akasaki Research</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">Akasaki Research</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/VisualDust" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:Miya@Akasaki.space" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/07/20/Disentangled-Non-Local-Neural-Networks/" title="「论文阅读笔记」Disentangled Non-Local Neural Networks">     <img class="post_bg" src="/img/cover/Disentangled-Non-Local-Neural-Networks.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」Disentangled Non-Local Neural Networks"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/20/Disentangled-Non-Local-Neural-Networks/" title="「论文阅读笔记」Disentangled Non-Local Neural Networks">「论文阅读笔记」Disentangled Non-Local Neural Networks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-20T13:49:37.000Z" title="发表于 2021-07-20 21:49:37">2021-07-20</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-20T14:54:02.955Z" title="更新于 2021-07-20 22:54:02">2021-07-20</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/attention/">attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/paper/">paper</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/spatial-wise/">spatial-wise</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/non-local/">non-local</a></span></div><div class="content">Disentangled Non-Local Neural Networks
Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, Han Hu

The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network. This paper first studies the non-local block in depth, where we find that its attention computation can be split into two terms, a whitened pairwise term accounting for the relationship between two pixels and a unary term representing the saliency of ev ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/07/20/Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/" title="「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond">     <img class="post_bg" src="/img/cover/Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/20/Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/" title="「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond">「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-20T13:46:12.000Z" title="发表于 2021-07-20 21:46:12">2021-07-20</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-20T14:41:00.416Z" title="更新于 2021-07-20 22:41:00">2021-07-20</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/attention/">attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/paper/">paper</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/spatial-wise/">spatial-wise</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/non-local/">non-local</a></span></div><div class="content">GCNet: Global Context Networks (Non-local Networks Meet Squeeze-Excitation Networks and Beyond)
Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, Han HuSubmitted on 25 Apr 2019
GCNet（原论文：GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond或Global Context Networks）这篇论文的研究思路类似于DPN，深入探讨了Non-local和SENet的优缺点，然后结合Non-local和SENet的优点提出了GCNet。

The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to  ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/07/20/Non-local-Neural-Networks/" title="「论文阅读笔记」Non local Neural Networks">     <img class="post_bg" src="/img/cover/Non-local-Neural-Networks.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」Non local Neural Networks"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/20/Non-local-Neural-Networks/" title="「论文阅读笔记」Non local Neural Networks">「论文阅读笔记」Non local Neural Networks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-20T13:42:28.000Z" title="发表于 2021-07-20 21:42:28">2021-07-20</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-20T14:31:49.910Z" title="更新于 2021-07-20 22:31:49">2021-07-20</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/attention/">attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/paper/">paper</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/spatial-wise/">spatial-wise</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/non-local/">non-local</a></span></div><div class="content">Non-local Neural Networks

The non-local block is a popular module for strengthening the context modeling ability of a regular convolutional neural network.

Non-local旨在使用单个Layer实现长距离的像素关系构建，属于自注意力（self-attention）的一种。常见的CNN或是RNN结构基于局部区域进行操作。例如，卷积神经网络中，每次卷积试图建立一定区域内像素的关系。但这种关系的范围往往较小（由于卷积核不大）。
为了建立像素之间的长距离依赖关系，也就是图像中非相邻像素点之间的关系，本文另辟蹊径，提出利用non-local operations构建non-local神经网络。这篇论文通过非局部操作解决深度神经网络核心问题：捕捉长距离依赖关系。

Both convolutional and recurrent operations are building blocks that process one local n ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/07/20/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation">     <img class="post_bg" src="/img/cover/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/20/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation">「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-20T13:35:30.000Z" title="发表于 2021-07-20 21:35:30">2021-07-20</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-20T14:41:22.939Z" title="更新于 2021-07-20 22:41:22">2021-07-20</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/attention/">attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/channal-wise/">channal-wise</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/paper/">paper</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/senet/">senet</a></span></div><div class="content">Rethinking BiSeNet For Real-time Semantic Segmentation
Mingyuan Fan, Shenqi Lai, Junshi Huang, Xiaoming Wei, Zhenhua Chai, Junfeng Luo, Xiaolin Wei


BiSeNet has been proved to be a popular two-stream network for real-time segmentation. However, its principle of adding an extra path to encode spatial information is time-consuming, and the backbones borrowed from pretrained tasks, e.g., image classification, may be inefficient for image segmentation due to the deficiency of task-specific design.  ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/07/19/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation">     <img class="post_bg" src="/img/cover/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/19/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation">「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-19T13:41:17.000Z" title="发表于 2021-07-19 21:41:17">2021-07-19</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-20T14:32:56.920Z" title="更新于 2021-07-20 22:32:56">2021-07-20</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/attention/">attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/channal-wise/">channal-wise</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/paper/">paper</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/senet/">senet</a></span></div><div class="content">BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation
BiSeNet的目标是更快速的实时语义分割。在语义分割任务中，空间分辨率和感受野很难两全，尤其是在实时语义分割的情况下，现有方法通常是利用小的输入图像或者轻量主干模型实现加速。但是小图像相较于原图像缺失了很多空间信息，而轻量级模型则由于裁剪通道而损害了空间信息。BiSegNet整合了Spatial Path (SP) 和 Context Path (CP)分别用来解决空间信息缺失和感受野缩小的问题。

Semantic segmentation requires both rich spatial information and sizeable receptive field. However, modern approaches usually compromise spatial resolution to achieve real-time inference speed, which leads to poor perform ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/07/19/Squeeze-and-Excitation-Networks/" title="「论文阅读笔记」SENet: Squeeze-and-Excitation Networks">     <img class="post_bg" src="/img/cover/Squeeze-and-Excitation-Networks.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」SENet: Squeeze-and-Excitation Networks"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/19/Squeeze-and-Excitation-Networks/" title="「论文阅读笔记」SENet: Squeeze-and-Excitation Networks">「论文阅读笔记」SENet: Squeeze-and-Excitation Networks</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-19T13:34:44.000Z" title="发表于 2021-07-19 21:34:44">2021-07-19</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-19T13:46:09.968Z" title="更新于 2021-07-19 21:46:09">2021-07-19</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/attention/">attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/channal-wise/">channal-wise</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/paper/">paper</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/senet/">senet</a></span></div><div class="content">Squeeze-and-Excitation Networks (SENet)
Squeeze-and-Excitation Networks（SENet）是由自动驾驶公司Momenta在2017年公布的一种全新的图像识别结构，它通过对特征通道间的相关性进行建模，把重要的特征进行强化来提升准确率。这个结构是2017 ILSVR竞赛的冠军，top5的错误率达到了2.251%，比2016年的第一名还要低25%。

The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investi ...</div></div></div><div class="recent-post-item"><div class="post_cover left_radius"><a href="/2021/07/19/attention-stuff/" title="Attention-注意力机制">     <img class="post_bg" src="/img/cover/attention-stuff.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Attention-注意力机制"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/19/attention-stuff/" title="Attention-注意力机制">Attention-注意力机制</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-19T13:13:41.000Z" title="发表于 2021-07-19 21:13:41">2021-07-19</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-19T13:40:09.318Z" title="更新于 2021-07-19 21:40:09">2021-07-19</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/attention/">attention</a><span class="article-meta__link">•</span><a class="article-meta__tags" href="/tags/survey/">survey</a></span></div><div class="content">注意力机制

目录
[[TOC]]
1. 介绍
注意力机制在很多AI领域内得到了成功的应用。这是人工神经网络在模仿人类进行决策过程的重要发展。

In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior.

上面这段文字摘自Alana de Santana Correia, and Esther Luna Colombini的论文 ATTENTION, PLEASE ! A SURVEY OF NEURAL ATTENTION MODELS IN DEEP LEARNING。你应该注意到了，在你的视野中，只有一部分区域是很清晰的。对于视野周围的场景，你往往需要转转眼珠，把视野朝向它， ...</div></div></div><div class="recent-post-item"><div class="post_cover right_radius"><a href="/2021/07/19/unlimited-paper-works/" title="Unlimited Paper Works">     <img class="post_bg" src="/img/background.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Unlimited Paper Works"></a></div><div class="recent-post-info"><a class="article-title" href="/2021/07/19/unlimited-paper-works/" title="Unlimited Paper Works">Unlimited Paper Works</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-19T12:26:47.000Z" title="发表于 2021-07-19 20:26:47">2021-07-19</time><span class="article-meta__separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-07-20T14:41:36.068Z" title="更新于 2021-07-20 22:41:36">2021-07-20</time></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fas fa-tag"></i><a class="article-meta__tags" href="/tags/paper/">paper</a></span></div><div class="content">欢迎来到魔法部日志
如果你看到了这里，说明你已经准备好开始探求这个领域事物的规律以及这些规律的本源了。作为一个本科生，最适合你入坑的就是开始习惯性阅读领域内论文。大约在我的大二的下半学期，我和我的朋友们开始共同阅读论文并写下笔记。这些笔记粗浅、幼稚，甚至会出现一些理解上的错误——万事开头难。但是我们还是想把这些笔记整理起来——这便是魔法部日志的开始。在我新建文件夹的时候，魔法部日志的文件夹名称是“unlimited paper works”，在成为理性的怀疑者之前，应该先掌握这个科研领域。我们做好了长期投入的准备，并希望把简单的事情做到出人意料得精彩。
加入魔法部日志也不是什么难事，你只需要热身一下，读完下面的一篇引导，就可以开始了(以下内容已通过语法检查工具PaperCube的检查)。
How to Read and Comprehend Scientific Research Articles
Scientific articles are how scholars and researchers communicate with each other. Reading scien ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/visualdust.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Miya Akasaki</div><div class="author-info__description">工具箱的魔法部日志</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/VisualDust"><i class="fab fa-github"></i><span>Github</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/VisualDust" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:Miya@Akasaki.space" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/07/20/Disentangled-Non-Local-Neural-Networks/" title="「论文阅读笔记」Disentangled Non-Local Neural Networks"><img src="/img/cover/Disentangled-Non-Local-Neural-Networks.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」Disentangled Non-Local Neural Networks"/></a><div class="content"><a class="title" href="/2021/07/20/Disentangled-Non-Local-Neural-Networks/" title="「论文阅读笔记」Disentangled Non-Local Neural Networks">「论文阅读笔记」Disentangled Non-Local Neural Networks</a><time datetime="2021-07-20T13:49:37.000Z" title="发表于 2021-07-20 21:49:37">2021-07-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/20/Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/" title="「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond"><img src="/img/cover/Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond"/></a><div class="content"><a class="title" href="/2021/07/20/Non-local-Networks-Meet-Squeeze-Excitation-Networks-and-Beyond/" title="「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond">「论文阅读笔记」GCNet：Non-local Networks Meet Squeeze-Excitation Networks and Beyond</a><time datetime="2021-07-20T13:46:12.000Z" title="发表于 2021-07-20 21:46:12">2021-07-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/20/Non-local-Neural-Networks/" title="「论文阅读笔记」Non local Neural Networks"><img src="/img/cover/Non-local-Neural-Networks.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」Non local Neural Networks"/></a><div class="content"><a class="title" href="/2021/07/20/Non-local-Neural-Networks/" title="「论文阅读笔记」Non local Neural Networks">「论文阅读笔记」Non local Neural Networks</a><time datetime="2021-07-20T13:42:28.000Z" title="发表于 2021-07-20 21:42:28">2021-07-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/20/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation"><img src="/img/cover/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation"/></a><div class="content"><a class="title" href="/2021/07/20/Rethinking-BiSeNet-For-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation">「论文阅读笔记」Rethinking BiSeNet For Real time Semantic Segmentation</a><time datetime="2021-07-20T13:35:30.000Z" title="发表于 2021-07-20 21:35:30">2021-07-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/07/19/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation"><img src="/img/cover/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation"/></a><div class="content"><a class="title" href="/2021/07/19/BiSeNet-Bilateral-Segmentation-Network-for-Real-time-Semantic-Segmentation/" title="「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation">「论文阅读笔记」BiSeNet Bilateral Segmentation Network for Real-time Semantic Segmentation</a><time datetime="2021-07-19T13:41:17.000Z" title="发表于 2021-07-19 21:41:17">2021-07-19</time></div></div></div></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/attention/" style="font-size: 1.45em; color: rgb(120, 158, 193)">attention</a><a href="/tags/channal-wise/" style="font-size: 1.3em; color: rgb(121, 56, 83)">channal-wise</a><a href="/tags/paper/" style="font-size: 1.45em; color: rgb(186, 3, 12)">paper</a><a href="/tags/senet/" style="font-size: 1.3em; color: rgb(19, 33, 82)">senet</a><a href="/tags/spatial-wise/" style="font-size: 1.3em; color: rgb(95, 108, 101)">spatial-wise</a><a href="/tags/non-local/" style="font-size: 1.3em; color: rgb(90, 192, 64)">non-local</a><a href="/tags/survey/" style="font-size: 1.15em; color: rgb(111, 95, 137)">survey</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2021/07/"><span class="card-archive-list-date">七月 2021</span><span class="card-archive-list-count">8</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">8</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2021-07-20T14:54:31.273Z"></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/background.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Miya Akasaki</div><div class="footer_custom_text">Akasaki research, more than interests.</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function subtitleType () {
  if (true) { 
    var typed = new Typed("#subtitle", {
      strings: "工具箱的『魔法部日志』".split(","),
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50
    })
  } else {
    document.getElementById("subtitle").innerHTML = '工'
  }
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.jsdelivr.net/npm/typed.js/lib/typed.min.js').then(subtitleType)
  }
} else {
  subtitleType()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>